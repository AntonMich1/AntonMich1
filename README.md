- üëã Hi, I‚Äôm @AntonMicheal
- üëÄ I‚Äôm interested in biology, mathematics, sport, finance and coding
- üå± Predictive modelling & machine learning approaches...
- üíûÔ∏è I‚Äôm looking to collaborate on ML projects
- üì´ Reach me antonilango@hotmail.com
- üòÑ Pronouns: JumpTheMouse
- ‚ö° Fun fact: Playing Volleyball, Biking, Solving puzzles

<!---
AntonMich1/AntonMich1 is a ‚ú® special ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.
You can click the Preview link to take a look at your changes.
--->
Projects
This section is a portfolio of Machine Learning projects with Python and various visualization and analysis tools. Most of these projects were carried out within the framework of IBM Data science certifications. They are presented with Jupyter Notebooks. I updated some projects by incorporating more in-depth data analysis, better graphs, advanced ML techniques.
* SpaceX Falcon 9 First Stage Landing Prediction
  
https://github.com/AntonMich1/testrepo/tree/main
In this project, we predict if the Falcon 9 first stage will land successfully. Project includes: SpaceX data collection, Data Wrangling, Webscraping, EDA with SQL Queries & Data visualization, SpaceX Launch Records Dashboard, Launch Sites Locations Analysis with Folium, Machine Learning classification with optimization of hyperparameters and selection of best model: KNN, Decision Tree, SVM, Logistic Regression.

Mathematics of Machine Learning. Most of these projects were carried out within the framework of Imperial College Online certifications. They are presented with Jupyter Notebooks. I updated some projects by incorporating more and more interesting questions
https://github.com/AntonMich1/Mathematics_MachineLearning

Ongoing Projects

Reinforcement Learning ‚Äì Temporal Difference (TD) Regularized Actor-Critic Model for Active Avoidance Learning

I am using parameters from cue-signaled active avoidance (AA) training in mice and feeding real learning data from 100 trials into an Actor-Critic reinforcement learning model. This model, with separate learning rates for action selection (actor) and state evaluation (critic), was applied to individual mice during avoidance acquisition.  
Latent parameters, such as learning rate and the subjective reinforcement value of foot shock, were extracted and compared across subjects. To enhance stability, I integrated Temporal Difference (TD) learning, which effectively handles delayed rewards and punishments‚Äîkey factors in avoidance behaviors. The TD error aligns with biological mechanisms like dopamine signaling, making it a plausible framework for understanding how animals learn to avoid threats. Additionally, the regularization term ensures stable and efficient learning, even in uncertain and complex environments.  
I am currently investigating whether the TD-regularized Actor-Critic RL model accurately represents and predicts avoidance behavior. I will provide updates as the project progresses.  

